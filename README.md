# Tutorial on Using TimeGAN to Generate Historical Stock Price Data
by Cydney Martell and Idris Sunmola

Generative Adversarial Networks (GANs) are a machine learning approach developed by Ian Goodfellow et al. in 2014 that use deep learning methods generate synthetic datasets. The overarching goal is to train a generator and discriminator in a competitive setting so the generator creates samples that the discriminator cannot distinguish from the real training data. GANs have been applied to generating high resolution images and in image to image translation. In addition to applications in image data, GANs have also been developed to generate synthetic time series datasets for applications in financial trading.  

In this tutorial, we will explain the TimeGAN architecture developed by Yoon et al. 2019 [1] and how it can be used to generate temporal time series data. We will then explain how to replicate TimeGAN and train it on our dataset to generate synthetic historical stock price dataset. The original manuscript can be found at the following link: https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf. 

Through this tutorial you will learn:
* The basics of TimeGAN architecture 
* How to set up the loss functions for TimeGAN 
* How to train the generator and discriminator 
* How to evaluate the quality of the synthetic time-series data generated by TimeGAN

## Tutorial Overview
1. Introduction to using GANs to generate temporal data
2. Historical stock prices dataset
3. Overview of TimeGAN architecture
4. Description of loss functions
5. How to train the generator
6. How to train the discriminator
8. Evaluation of training results
9. Conclusions and future applications

## Introduction to using GANs to generate temporal data
There is a need to generate synthetic time series data sets to augment these datasets for applications in financial trading and the medical field [2]. Data augmentation of time series data sets could be especially useful in scenarios where privacy is an issue and to reduce backtest overfitting in models trained on historical time series data which is scarce [3]. Developing a model to generate synthetic time series data is particularly challenging because the model needs to learn both the feature distributions at a specific time point and the dynamics of these features across time. 

In the mauscript, Yoon et.al summarize and compare their TimeGAN architecture to other models developed to generate synthetic time series data.
  1. Autoregressive Recurrent Networks
  2. GAN Based Approaches <br>
**FINISH THIS**

## Historical stock prices dataset
We trained our replicated TimeGAN architechture on a free dataset of end of day stock prices from https://www.quandl.com/databases/WIKIP/documentation. This is a large dataset that contains the end of the day stock prices collected everyday from July 6, 2015 to 2018. While this set contains data for 3,000 US companies,  we decided to select 86 tickers from this dataset for our replication of TimeGAN. The list of tickers that we used can be found in the file tickers.txt. 

The following image shows an example of the historical price data for 8 of the tickers. 
![image](https://user-images.githubusercontent.com/78554498/110181326-23f7a080-7dd1-11eb-83ba-58be8b3b13c6.png)

Prior to training our TimeGAN model, we preprocessed the data following the methods of preprocess used by Yoon et. al. We used MinMaxScaler to scale the raw price series data between 0 and 1. We then created rolling window sequences with an overlap of 24 data points as used by Yoon et al. 
    
    #Obtains and stores the historical price dataset
    def get_wiki_prices():
        #Downloaded from https://www.quandl.com/api/v3/datatables/WIKI/PRICES?qopts.export=true&api_key=<API_KEY> and saved as 'Wiki.csv'
      

        df = pd.read_csv('Wiki.csv',
                         parse_dates=['date'],
                         index_col=['date', 'ticker'],
                         infer_datetime_format=True) 
        with pd.HDFStore('assets.h5') as store:
            store.put('quandl/wiki/prices', df)
            
    #Generates a list of ticker names      
    with open("tickers.txt") as file:
       tickers = [line.strip() for line in file]
   
    #Selects the adjusted close prices from the dataset for the 86 tickers of interest        
    def select_data():
        df = (pd.read_hdf('assets.h5', 'quandl/wiki/prices')
              .adj_close
              .unstack('ticker')
              .loc['2000':, tickers]
              .dropna())
            df.to_hdf(hdf_store, 'data/real')
    
    #Normalizes the stock price data to be between 0 and 1 
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df).astype(np.float32)
    
    #Creates rolling window sequences
    seq_len = 24
    data = []
    for i in range(len(df) - seq_len):
         data.append(scaled_data[i:i + seq_len])

## Overview of TimeGAN architecture
Generative Adversarial Networks have come a long way since their initial unveiling by Ian Goodfellow of Google Brain at the 2016 NeurIPS conference. There have been countless iterations of GANs with just as many intricate architectures to match. GANs has become a staple in the machine learning arena, not only for its awe inspiring results, but also for its sheer utility.

The first generation of generative architectures had 2 components that literally “duked it out” in a sort of computational game of deception. One of its components, the discriminator, is nothing more than a deep supervised learning classifier that is well known and understood by anyone that has dabbled in any capacity in ML. The other component, the generator, is more of an enigma (,at least during its inauguration,) due to its relative complexity. The generator, unlike its discriminatory counterpart, takes in a vector drawn from a multivariate standard normal distribution. For the initiated, this is no different from the decoder of a variational autoencoder.

The goal is to map from a latent space back to the original domain. The generator aims to stump the discriminator by generating images that are so similar to the original class of images that they could have been drawn from the same dataset.

![gedl_0407](https://user-images.githubusercontent.com/20098178/110197863-ef590880-7e13-11eb-84c3-a94cb5f97cce.png)

Harkening back to what we said about the intricacies of the new crop of generative models, the TimeGan architecture is a testament to this fact. Consisting of four network modules, TimeGan is evenly split into an autoencoding component and adversarial component. With the autoencoder consisting of an embedding function and a recovery function. While the adversarial component comprises a sequence generator and a sequence discriminator.

The autoencoder’s embedding and recovery functions provide the mappings between the feature and latent space. Via lower-dimensional representations, the autoencoder allows the adversarial network to learn some underlying temporal dynamic of the data. Below is the embedding function e : 

![mathpix 2021-03-05 23-36-48](https://user-images.githubusercontent.com/20098178/110198066-50cda700-7e15-11eb-91bc-ba55ab3b2faf.png)

In the case of the generator, it first outputs into the embedding space, then produces the synthetic output directly in the feature space. Below is the generator function g :

![mathpix 2021-03-06 00-24-06](https://user-images.githubusercontent.com/20098178/110198176-1d3f4c80-7e16-11eb-80ef-993f64ed9518.png)



![gan_arch](https://user-images.githubusercontent.com/20098178/110197437-44474f80-7e11-11eb-848c-87ac07044a69.png)




## Description of loss functions

## How to train the generator

## How to train the discriminator

## Evaluation of training results
After training the model, we were able to generate synthetic stock price data for 84 tickers. 


Now we want to assess the quality of this synthetic data to evaluate our replication of the TimeGAN architechture. We will use the same criteria as Yoon et. al to evaluate the quality of the synthetic time series data generated by our model. The three criteria will will use are:
  1. Diversity
  2. Fidelity
  3. Usefulness 
 
### Diversity
First, we will investigate the diversity of the dataset. The main question we are asking here is: does the synthetic data distribution match the distribution of the real data? 


### Fidelity 
Next, we will investigate the fidelity of the dataset. Primarly we want to know, is the synthetic price series indistinguishable from the real data. 

### Usefulness
Lastly, we will look at the usefulness of the synthetic data. So, we want to know is the synthetic data series as useful as the real data for solving a predictive task. 


## Conclusions and future applications

## References
[1] https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf <br>
[2] https://arxiv.org/pdf/2002.12478.pdf <br>
[3] http://www.blackarbs.com/synthetic-data
